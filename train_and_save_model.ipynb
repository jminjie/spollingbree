{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8965b5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n",
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "# TODO based on https://www.tensorflow.org/text/tutorials/text_generation\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f614668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 4234930 characters\n",
      "*polyciliate**telemetrograph**trionfo**exclaimer**colotomy**waistless**anthramine**undupable**anthomedusan**jargonising**untruthfulness**fugacious**unluckiest**clatch**singfo**maronian**logogriph**spooky**induplicate**hexaradial**acrodont**gambia**st\n"
     ]
    }
   ],
   "source": [
    "text = open('words_alpha.txt', 'rb').read().decode(encoding='utf-8')\n",
    "text = '*' + ('**'.join(text.split())) + '*'\n",
    "\n",
    "print(f'Length of text: {len(text)} characters')\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc89e558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 unique characters\n",
      "['*', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99993d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 14:24:56.605902: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-01-17 14:24:56.606088: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "ids_from_chars = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)\n",
    "\n",
    "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "\n",
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fdfc058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(17, shape=(), dtype=int64) p\n",
      "tf.Tensor(16, shape=(), dtype=int64) o\n",
      "tf.Tensor(13, shape=(), dtype=int64) l\n",
      "tf.Tensor(26, shape=(), dtype=int64) y\n",
      "tf.Tensor(4, shape=(), dtype=int64) c\n",
      "tf.Tensor(10, shape=(), dtype=int64) i\n",
      "tf.Tensor(13, shape=(), dtype=int64) l\n",
      "tf.Tensor(10, shape=(), dtype=int64) i\n",
      "tf.Tensor(2, shape=(), dtype=int64) a\n"
     ]
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "\n",
    "for element in ids_dataset.take(10):\n",
    "    print(element, chars_from_ids(element).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc14ed78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1 17 16 13 26  4 10 13 10  2 21  6  1  1 21  6], shape=(16,), dtype=int64) tf.Tensor(b'*polyciliate**te', shape=(), dtype=string)\n",
      "tf.Tensor([13  6 14  6 21 19 16  8 19  2 17  9  1  1 21 19], shape=(16,), dtype=int64) tf.Tensor(b'lemetrograph**tr', shape=(), dtype=string)\n",
      "tf.Tensor([10 16 15  7 16  1  1  6 25  4 13  2 10 14  6 19], shape=(16,), dtype=int64) tf.Tensor(b'ionfo**exclaimer', shape=(), dtype=string)\n",
      "tf.Tensor([ 1  1  4 16 13 16 21 16 14 26  1  1 24  2 10 20], shape=(16,), dtype=int64) tf.Tensor(b'**colotomy**wais', shape=(), dtype=string)\n",
      "tf.Tensor([21 13  6 20 20  1  1  2 15 21  9 19  2 14 10 15], shape=(16,), dtype=int64) tf.Tensor(b'tless**anthramin', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# TODO if we don't need a regular sequence length maybe we could make sequences\n",
    "# always start and end at '*'.\n",
    "# Not sure if this matters since we always prompt with * later when running the model\n",
    "\n",
    "# read karpathi to see how he chooses sequence length and if it's important\n",
    "# maybe longer is better? but we don't want to keep context between words\n",
    "\n",
    "# try truncating and padding after the stop token\n",
    "# sequence length = shortest word length, change the start character into _\n",
    "\n",
    "seq_length = 15\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(5):\n",
    "  print(seq, text_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f34ac73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'*polyciliate**t'\n",
      "Target: b'polyciliate**te'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 14:24:57.979420: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-01-17 14:24:57.979570: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "dataset = sequences.map(split_input_target)\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b9ba5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658a9eb6",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "We use an RNN with GRUs defined in rnn_gru_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "375aa43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn_gru_model import RnnGRUModel\n",
    "model = RnnGRUModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03562a58",
   "metadata": {},
   "source": [
    "### Try the untrained model on the first sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3291c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 15, 28) # (batch_size, sequence_length, vocab_size)\n",
      "Model: \"rnn_gru_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  7168      \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    multiple                  3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  28700     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  812       \n",
      "=================================================================\n",
      "Total params: 3,974,984\n",
      "Trainable params: 3,974,984\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "    \n",
    "# vocab size is 28 because of [UNK]\n",
    "# TODO we don't actually need [UNK] since input involves all possible characters already\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "688c492b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 25, 23,  7, 19, 19, 12,  4, 16, 27, 14, 14,  5,  7, 14])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5d70caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " corn**larded**h\n",
      "Next Char Predictions:\n",
      " ixvfrrkcozmmdfm\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy().decode('utf-8'))\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e06ecc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 15, 28)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         3.3311896\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "mean_loss = example_batch_loss.numpy().mean()\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", mean_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "629e45a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.971594"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A newly initialized model shouldn't be too sure of itself, the output logits should\n",
    "# all have similar magnitudes. To confirm this you can check that the exponential of the mean\n",
    "# loss is approximately equal to the vocabulary size. A much higher loss means the model is\n",
    "# sure of its wrong answers, and is badly initialized:\n",
    "\n",
    "tf.exp(mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99d48eb",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d4411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095b6821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = 'training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42160b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e2e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10c3f72",
   "metadata": {},
   "source": [
    "### Create wrapper which generates and evaluates word plausiblity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6c8dc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).embedding.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).gru.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).gru.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).gru.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).embedding.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).gru.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).gru.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).gru.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "from rnn_plausiblewords import RnnWordPlausibilityEvaluator\n",
    "import logging\n",
    "model_wrapper = RnnWordPlausibilityEvaluator(logging, temperature=1)\n",
    "#                                             model=model,\n",
    "#                                             ids_from_chars=ids_from_chars,\n",
    "#                                             chars_from_ids=chars_from_ids,\n",
    "#                                             temperature=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cd224d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**ayuse*\n",
      "**upweed*\n",
      "**chastralite*\n",
      "**petalouportionazeal*\n",
      "**social*\n"
     ]
    }
   ],
   "source": [
    "# TODO generate some words using Spolling Bree letters and see how the models agree or disagree\n",
    "def create_words(given_model, num_words):\n",
    "    for m in range(num_words):\n",
    "      states = None\n",
    "      next_char = tf.constant(['**'])\n",
    "      result = [next_char]\n",
    "\n",
    "      for n in range(100):\n",
    "        next_char, states = given_model.generate_one_step(next_char, states=states)\n",
    "        result.append(next_char)\n",
    "        if next_char == '*':\n",
    "          break\n",
    "\n",
    "      result = tf.strings.join(result)\n",
    "      end = time.time()\n",
    "      print(result[0].numpy().decode('utf-8'))\n",
    "\n",
    "create_words(model_wrapper, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce2b48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrapper.evaluate_word('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf973b2",
   "metadata": {},
   "source": [
    "### Save and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1303cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the model weights only\n",
    "model.save_weights(\"base_model_saved_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08901b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the layers needed to regenerate ModelWrapper\n",
    "tf.saved_model.save(chars_from_ids, 'chars_from_ids')\n",
    "tf.saved_model.save(ids_from_chars, 'ids_from_chars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde07a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model.load_weights(\"base_model_saved_weights\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow25] *",
   "language": "python",
   "name": "conda-env-tensorflow25-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
