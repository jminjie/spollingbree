{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8965b5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "# based on https://www.tensorflow.org/text/tutorials/text_generation\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f614668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 11025662 characters\n",
      "_aahed************************_aahing***********************_aahs*************************_aalii************************_aaliis***********************_aals*************************_aani*************************_aardvark*********************_aardvarks********************_aardwolf*********************\n"
     ]
    }
   ],
   "source": [
    "full_text = open('words_alpha.txt', 'rb').read().decode(encoding='utf-8')\n",
    "text = ''\n",
    "seq_length = 30\n",
    "for word in full_text.split():\n",
    "    if len(word) < 4:\n",
    "        continue\n",
    "    else:\n",
    "        word = '_' + word\n",
    "        for i in range(0, seq_length-len(word)):\n",
    "            word = word + '*'\n",
    "    text = text + word\n",
    "\n",
    "print(f'Length of text: {len(text)} characters')\n",
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc89e558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 unique characters\n",
      "['*', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99993d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-27 12:24:28.843977: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-01-27 12:24:28.844231: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "ids_from_chars = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)\n",
    "\n",
    "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "\n",
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fdfc058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2, shape=(), dtype=int64) _\n",
      "tf.Tensor(3, shape=(), dtype=int64) a\n",
      "tf.Tensor(3, shape=(), dtype=int64) a\n",
      "tf.Tensor(10, shape=(), dtype=int64) h\n",
      "tf.Tensor(7, shape=(), dtype=int64) e\n",
      "tf.Tensor(6, shape=(), dtype=int64) d\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n"
     ]
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "\n",
    "for element in ids_dataset.take(seq_length):\n",
    "    print(element, chars_from_ids(element).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc14ed78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 2  3  3 10  7  6  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1], shape=(30,), dtype=int64) tf.Tensor(b'_aahed************************', shape=(), dtype=string)\n",
      "tf.Tensor(\n",
      "[ 2  3  3 10 11 16  9  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1], shape=(30,), dtype=int64) tf.Tensor(b'_aahing***********************', shape=(), dtype=string)\n",
      "tf.Tensor(\n",
      "[ 2  3  3 10 21  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1], shape=(30,), dtype=int64) tf.Tensor(b'_aahs*************************', shape=(), dtype=string)\n",
      "tf.Tensor(\n",
      "[ 2  3  3 14 11 11  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1], shape=(30,), dtype=int64) tf.Tensor(b'_aalii************************', shape=(), dtype=string)\n",
      "tf.Tensor(\n",
      "[ 2  3  3 14 11 11 21  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1], shape=(30,), dtype=int64) tf.Tensor(b'_aaliis***********************', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# TODO read karpathi to see how he chooses sequence length and if it's important\n",
    "# maybe longer is better? but we don't want to keep context between words\n",
    "\n",
    "# try truncating and padding after the stop token\n",
    "# sequence length = shortest word length, change the start character into _\n",
    "\n",
    "sequences = ids_dataset.batch(seq_length, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(5):\n",
    "  print(seq, text_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f34ac73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"args_0:0\", shape=(30,), dtype=int64)\n",
      "Input : b'_aahed************************'\n",
      "Target: b'aahed*************************'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-27 12:24:36.949568: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-01-27 12:24:36.949926: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "def split_input_target(sequence):\n",
    "    print(sequence)\n",
    "    input_text = sequence\n",
    "    target_text = tf.concat([sequence[1:], [1]], 0) # pad with extra *\n",
    "    return input_text, target_text\n",
    "dataset = sequences.map(split_input_target)\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b9ba5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658a9eb6",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "We use an RNN with GRUs defined in rnn_gru_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "375aa43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn_gru_model import RnnGRUModel\n",
    "model = RnnGRUModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03562a58",
   "metadata": {},
   "source": [
    "### Try the untrained model on the first sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3291c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 30, 29) # (batch_size, sequence_length, vocab_size)\n",
      "Model: \"rnn_gru_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  7424      \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    multiple                  3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  29725     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  870       \n",
      "=================================================================\n",
      "Total params: 3,976,323\n",
      "Trainable params: 3,976,323\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "    \n",
    "# vocab size is 29 because of [UNK]\n",
    "# TODO we don't actually need [UNK] since input involves all possible characters already\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "688c492b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  4,  2, 15, 23,  4, 26, 16,  6, 17, 23, 23, 19, 15,  9, 10, 20,\n",
       "       26,  0, 10, 17,  0, 16,  9, 28, 18, 21, 25, 16, 16])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5d70caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " _accidents********************\n",
      "Next Char Predictions:\n",
      " cb_mubxndouuqmghrx[UNK]ho[UNK]ngzpswnn\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy().decode('utf-8'))\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e06ecc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 30, 29)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         3.3777168\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "mean_loss = example_batch_loss.numpy().mean()\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "629e45a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.303791"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A newly initialized model shouldn't be too sure of itself, the output logits should\n",
    "# all have similar magnitudes. To confirm this you can check that the exponential of the mean\n",
    "# loss is approximately equal to the vocabulary size. A much higher loss means the model is\n",
    "# sure of its wrong answers, and is badly initialized:\n",
    "\n",
    "tf.exp(mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99d48eb",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51d4411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "095b6821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = 'training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42160b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82e2e6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-27 12:25:09.237448: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-27 12:25:09.502257: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-27 12:25:09.589565: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5742/5742 [==============================] - 801s 139ms/step - loss: 0.6392\n",
      "Epoch 2/3\n",
      "5742/5742 [==============================] - 805s 140ms/step - loss: 0.5682\n",
      "Epoch 3/3\n",
      "5742/5742 [==============================] - 810s 141ms/step - loss: 0.5676\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10c3f72",
   "metadata": {},
   "source": [
    "### Create wrapper which generates and evaluates word plausiblity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a6c8dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn_plausiblewords import RnnWordPlausibilityEvaluator\n",
    "import logging\n",
    "model_wrapper = RnnWordPlausibilityEvaluator(logging,\n",
    "                                             model=model,\n",
    "                                             ids_from_chars=ids_from_chars,\n",
    "                                             chars_from_ids=chars_from_ids,\n",
    "                                             temperature=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6cd224d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_udex*\n",
      "_coller*\n",
      "_elvies*\n",
      "_agmanies*\n",
      "_commonied*\n",
      "_compic*\n",
      "_cands*\n",
      "_candological*\n",
      "_dgorm*\n",
      "_oxyphyl*\n"
     ]
    }
   ],
   "source": [
    "# TODO generate some words using Spolling Bree letters and see how the models agree or disagree\n",
    "def create_words(given_model, num_words):\n",
    "    for m in range(num_words):\n",
    "      states = None\n",
    "      next_char = tf.constant(['_'])\n",
    "      result = [next_char]\n",
    "\n",
    "      for n in range(100):\n",
    "        next_char, states = given_model.generate_one_step(next_char, states=states)\n",
    "        result.append(next_char)\n",
    "        if next_char == '*':\n",
    "          break\n",
    "\n",
    "      result = tf.strings.join(result)\n",
    "      end = time.time()\n",
    "      print(result[0].numpy().decode('utf-8'))\n",
    "\n",
    "create_words(model_wrapper, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2ce2b48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.7846328020095825\n",
      "24.958137571811676\n"
     ]
    }
   ],
   "source": [
    "print(model_wrapper.evaluate_word('test'))\n",
    "print(model_wrapper.evaluate_word('testestest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf973b2",
   "metadata": {},
   "source": [
    "### Save and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1303cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the model weights only\n",
    "model.save_weights(\"base_model_saved_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e08901b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: chars_from_ids/assets\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: ids_from_chars/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-18 14:41:24.907000: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "## Save the layers needed to regenerate ModelWrapper\n",
    "tf.saved_model.save(chars_from_ids, 'chars_from_ids')\n",
    "tf.saved_model.save(ids_from_chars, 'ids_from_chars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde07a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model.load_weights(\"base_model_saved_weights\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow25] *",
   "language": "python",
   "name": "conda-env-tensorflow25-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
