{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "add59145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "342e6df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model (this must stay in sync with train_and_save_model.py)\n",
    "class RnnGRUModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense1 = tf.keras.layers.Dense(vocab_size)\n",
    "    self.dense2 = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense1(x, training=training)\n",
    "    x = self.dense2(x, training=training)\n",
    "    \n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6975c193",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "Metal device set to: Apple M1\n",
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 10:36:55.703914: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-01-17 10:36:55.704039: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-01-17 10:36:55.722943: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-01-17 10:36:55.723383: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-01-17 10:36:55.723439: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-17 10:36:55.740241: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# load other needed layers\n",
    "ids_from_chars = tf.keras.models.load_model('ids_from_chars')\n",
    "chars_from_ids = tf.keras.models.load_model('chars_from_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7b5fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the base model\n",
    "vocab_size = 28\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "loaded_model = RnnGRUModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)\n",
    "\n",
    "# load the saved weights\n",
    "loaded_model.load_weights(\"base_model_saved_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc71fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model wrapper which provides utilities for running the model and doing word evaluation\n",
    "# (this must stay in sync with train_and_save_model.py)\n",
    "# TODO actually this should live in its own file\n",
    "class ModelWrapper():\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=0.5):\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states\n",
    "\n",
    "  def probability_of_letter(this, logits, letter):\n",
    "    return (tf.squeeze(logits)[this.ids_from_chars(letter).numpy()]).numpy()\n",
    "\n",
    "  def evaluate_word(this, word, show_work):\n",
    "      # Seed the model with '**'\n",
    "      seed_chars = tf.constant(['**'])\n",
    "      input_chars = tf.strings.unicode_split(seed_chars, 'UTF-8')\n",
    "      input_ids = this.ids_from_chars(input_chars).to_tensor()\n",
    "      states = None\n",
    "\n",
    "      # get the initial probability distribution\n",
    "      predicted_logits, states = this.model(inputs=input_ids, states=states, return_state=True)\n",
    "      predicted_logits = predicted_logits[:, -1, :]/this.temperature\n",
    "\n",
    "      total_prob = 0\n",
    "      for letter in word:\n",
    "        # check the log probability of the letter given current state\n",
    "        probability = this.probability_of_letter(predicted_logits, letter)\n",
    "        total_prob += probability\n",
    "        if show_work:\n",
    "          print(\"Probability of\", letter, \"=\", probability)\n",
    "\n",
    "        # feed this letter into the model\n",
    "        this_char = tf.constant([letter])        \n",
    "        input_char = tf.strings.unicode_split(this_char, 'UTF-8')\n",
    "        this_input_id = this.ids_from_chars(input_char).to_tensor()\n",
    "        predicted_logits, states = this.model(inputs=this_input_id, states=states,\n",
    "                                              return_state=True)\n",
    "        predicted_logits = predicted_logits[:, -1, :]/this.temperature\n",
    "\n",
    "      # check the log probability of '*' given current state\n",
    "      probability = this.probability_of_letter(predicted_logits, '*')\n",
    "      # TODO This assumes that the probability of the word is the product of the probability of each letter\n",
    "      # the problem is that I'm not really training the RNN to generate probabilities for each letter. In otherwords\n",
    "      # the log likelihoods in the logits is not real, just directionally correct\n",
    "      total_prob += probability\n",
    "      if show_work:\n",
    "        print(\"Probability of * =\", probability)\n",
    "\n",
    "      # TODO normalize by length\n",
    "      # TODO actually, shouldn't the model do this automatically if it's well trained?\n",
    "      return total_prob\n",
    "\n",
    "# initialize the model wrapper\n",
    "model_wrapper = ModelWrapper(loaded_model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20588d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of t = 0.939692\n",
      "Probability of e = 7.774877\n",
      "Probability of s = 3.583509\n",
      "Probability of t = 10.354789\n",
      "Probability of * = 5.4483023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28.101169109344482"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wrapper.evaluate_word('test', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca4d51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow25] *",
   "language": "python",
   "name": "conda-env-tensorflow25-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
