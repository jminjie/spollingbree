{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "136413c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "2.5.0\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bf9d7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 175419 characters\n",
      "**the**of**and**to**a**in**for**is**on**that**by**this**with**i**you**it**not**or**be**are**from**at**as**your**all**have**new**more**an**was**we**will**home**can**us**about**if**page**my**has**search**free**but**our**one**other**do**no**information*\n"
     ]
    }
   ],
   "source": [
    "text = open('20k.txt', 'rb').read().decode(encoding='utf-8')\n",
    "text = '**' + ('**'.join(text.split())) + '*'\n",
    "\n",
    "print(f'Length of text: {len(text)} characters')\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be6c5319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 unique characters\n",
      "['*', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80c1312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO try setting mask_token = 'c' and see what the output looks like\n",
    "\n",
    "ids_from_chars = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)\n",
    "\n",
    "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "\n",
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "004730c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(21, shape=(), dtype=int64) t\n",
      "tf.Tensor(9, shape=(), dtype=int64) h\n",
      "tf.Tensor(6, shape=(), dtype=int64) e\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n",
      "tf.Tensor(16, shape=(), dtype=int64) o\n",
      "tf.Tensor(7, shape=(), dtype=int64) f\n",
      "tf.Tensor(1, shape=(), dtype=int64) *\n"
     ]
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "\n",
    "for element in ids_dataset.take(10):\n",
    "    print(element, chars_from_ids(element).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc8b4060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1  1 21  9  6  1  1 16  7  1  1  2 15  5  1  1], shape=(16,), dtype=int64) tf.Tensor(b'**the**of**and**', shape=(), dtype=string)\n",
      "tf.Tensor([21 16  1  1  2  1  1 10 15  1  1  7 16 19  1  1], shape=(16,), dtype=int64) tf.Tensor(b'to**a**in**for**', shape=(), dtype=string)\n",
      "tf.Tensor([10 20  1  1 16 15  1  1 21  9  2 21  1  1  3 26], shape=(16,), dtype=int64) tf.Tensor(b'is**on**that**by', shape=(), dtype=string)\n",
      "tf.Tensor([ 1  1 21  9 10 20  1  1 24 10 21  9  1  1 10  1], shape=(16,), dtype=int64) tf.Tensor(b'**this**with**i*', shape=(), dtype=string)\n",
      "tf.Tensor([ 1 26 16 22  1  1 10 21  1  1 15 16 21  1  1 16], shape=(16,), dtype=int64) tf.Tensor(b'*you**it**not**o', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# TODO if we don't need a regular sequence length maybe we could make sequences\n",
    "# always start and end at '*'.\n",
    "# Not sure if this matters since we always prompt with * later when running the model\n",
    "\n",
    "# read karpathi to see how he chooses sequence length and if it's important\n",
    "# maybe longer is better? but we don't want to keep context between words\n",
    "\n",
    "# try truncating and padding after the stop token\n",
    "\n",
    "# try training on only 2 letter words \n",
    "\n",
    "seq_length = 15\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(5):\n",
    "  print(seq, text_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4ad559c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'**the**of**and*'\n",
      "Target: b'*the**of**and**'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-15 11:52:08.638928: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-01-15 11:52:08.639197: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "dataset = sequences.map(split_input_target)\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "591d2b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[20, 10,  8, 15,  6,  5,  1,  1,  2, 13, 16, 15,  8,  1,  1],\n",
       "         [ 4, 21, 10,  4,  6, 20,  1,  1, 20, 16, 19, 21,  6,  5,  1],\n",
       "         [20, 21,  6, 19, 20,  1,  1,  2, 20, 21,  6, 19, 10, 20, 12],\n",
       "         [22, 13, 13, 26,  1,  1,  7,  2, 21,  9,  6, 19,  1,  1,  6],\n",
       "         [15,  6, 15, 20,  1,  1, 20, 21,  6, 23,  6, 15, 20, 16, 15],\n",
       "         [19, 19,  6,  4, 21, 15,  6, 20, 20,  1,  1, 24,  6, 20, 21],\n",
       "         [15, 10, 20,  9,  6,  5,  1,  1, 11, 16, 15,  2, 20,  1,  1],\n",
       "         [ 1,  1,  7,  2, 23, 16, 19,  1,  1, 23,  6, 21,  6, 19,  2],\n",
       "         [12,  4,  1,  1,  4, 19,  2, 20,  9,  6, 20,  1,  1, 20, 21],\n",
       "         [ 1, 13, 16,  2,  5, 20,  1,  1,  7, 19, 10,  6, 15,  5, 20],\n",
       "         [ 9, 10, 19,  6,  1,  1, 17, 16, 17, 22, 13,  2, 21, 10, 16],\n",
       "         [ 1,  1,  3,  2,  4, 12, 20,  1,  1, 16,  3, 20,  6, 19, 23],\n",
       "         [13, 16, 16, 12,  6,  5,  1,  1,  5, 10, 20,  4, 22, 20, 20],\n",
       "         [25,  1,  1,  4,  9, 10, 15,  2,  1,  1, 14,  2, 12, 10, 15],\n",
       "         [ 4, 16, 19, 15,  6, 19,  1,  1, 19,  2, 15, 12,  1,  1,  4],\n",
       "         [ 1,  4, 16, 16, 12,  6,  5,  1,  1, 21,  9, 19, 10, 13, 13],\n",
       "         [21,  6, 19, 20, 16, 15,  1,  1, 19,  2, 26, 20,  1,  1,  2],\n",
       "         [21, 19,  2, 15, 20, 17, 16, 19, 21,  6, 19,  1,  1, 11, 16],\n",
       "         [ 8,  1,  1, 20, 16, 15, 16, 14,  2,  1,  1,  6, 20, 21,  2],\n",
       "         [ 1,  5, 16, 24, 15, 13, 16,  2,  5,  1,  1,  9,  1,  1,  9],\n",
       "         [21, 10, 16, 15,  1,  1, 20, 13, 16, 21, 20,  1,  1, 10, 15],\n",
       "         [ 1, 20, 10, 15,  8,  2, 17, 16, 19,  6,  1,  1,  6, 15, 21],\n",
       "         [ 4,  9, 22,  3,  3, 26,  1,  1,  8, 19,  2, 23,  6,  1,  1],\n",
       "         [26,  1,  1, 20, 15,  2, 12,  6, 20,  1,  1, 20, 24,  6, 13],\n",
       "         [ 6, 15,  1,  1,  4, 16, 17, 10,  6, 19,  1,  1, 22, 15,  4],\n",
       "         [20,  1,  1, 21, 19, 22, 20, 21,  1,  1, 23,  2, 15,  1,  1],\n",
       "         [16, 23,  6, 15, 10,  2, 15,  1,  1, 17, 19,  2,  4, 21, 10],\n",
       "         [19, 21, 10, 13, 10, 21, 26,  1,  1,  3, 26, 19,  5,  1,  1],\n",
       "         [16, 15,  1,  1,  7, 16, 22, 15,  5, 10, 15,  8,  1,  1, 20],\n",
       "         [20,  1,  1, 17,  2,  4,  6,  5,  1,  1,  6, 15,  8, 13,  1],\n",
       "         [10, 15,  1,  1, 24, 10, 12, 10, 17,  6,  5, 10,  2,  1,  1],\n",
       "         [13,  6,  1,  1,  8, 22,  2, 19,  5, 20,  1,  1,  2, 15, 15],\n",
       "         [10, 20, 15, 21,  1,  1, 24,  2, 23,  6,  7, 16, 19, 14,  1],\n",
       "         [16, 19,  1,  1, 20, 21,  2, 19,  1,  1,  2, 19,  6,  2, 20],\n",
       "         [ 6, 19, 21, 10, 15,  8,  1,  1, 21, 10,  3,  6, 21,  2, 15],\n",
       "         [ 1,  1,  5, 16, 19, 16, 21,  9, 26,  1,  1,  7, 19,  6,  6],\n",
       "         [20, 17, 13,  2,  4,  6,  5,  1,  1, 20, 16, 22, 15,  5,  6],\n",
       "         [ 1,  1, 19,  6,  4, 16, 14,  3, 10, 15,  2, 15, 21,  1,  1],\n",
       "         [23, 10,  5,  6,  1,  1, 11,  1,  1,  7, 16, 16,  5,  1,  1],\n",
       "         [ 5,  1,  1,  3,  6,  9,  2, 23, 10, 16, 19,  2, 13,  1,  1],\n",
       "         [ 2, 25,  6, 20,  1,  1, 17, 16, 20, 21,  7, 10, 25,  1,  1],\n",
       "         [19,  4,  9,  2,  3, 13,  6,  1,  1, 21, 16, 22,  4,  9, 10],\n",
       "         [ 1,  1, 23, 10, 23,  2,  1,  1, 21, 19,  6,  2,  5, 14, 10],\n",
       "         [ 5,  6, 19,  1,  1, 10, 13, 13, 10, 15, 16, 10, 20,  1,  1],\n",
       "         [ 1,  5,  6, 14,  6, 15, 21, 10,  2,  1,  1,  3,  2, 19, 13],\n",
       "         [ 4,  9, 19, 10, 20,  1,  1,  2, 21, 21,  6, 15, 21, 10, 16],\n",
       "         [ 1, 19, 10,  5,  6, 19, 20,  1,  1,  4,  9,  2, 19,  8,  6],\n",
       "         [ 2,  3, 13,  6,  1,  1,  2, 13, 13, 16,  4,  2, 21, 10, 16],\n",
       "         [ 1,  1,  2, 15, 21, 10,  8, 22,  2,  1,  1, 15, 16, 21, 21],\n",
       "         [ 2, 20,  1,  1,  4, 16, 15, 15,  6,  4, 21, 16, 19, 20,  1],\n",
       "         [ 4,  9, 10, 15,  8,  1,  1, 20, 17, 16, 19, 21, 10, 15,  8],\n",
       "         [15,  8, 20,  1,  1, 10, 14, 20,  1,  1, 14, 16, 10, 20, 21],\n",
       "         [19, 10,  3, 22, 21, 10, 16, 15,  1,  1, 14,  6,  2, 20, 22],\n",
       "         [ 4, 13,  6,  1,  1,  7, 22, 19, 15, 10, 20,  9, 10, 15,  8],\n",
       "         [ 1,  1, 21, 10,  1,  1,  7,  4,  1,  1,  8,  6, 13,  1,  1],\n",
       "         [ 9, 21,  9, 16, 22, 20,  6,  1,  1, 17, 19, 16, 23,  6, 20],\n",
       "         [ 6,  5,  1,  1,  2, 13,  3,  2,  1,  1, 22, 15,  5,  6, 19],\n",
       "         [10, 15,  8,  1,  1,  4, 16, 14, 17, 22, 21,  6, 19, 10, 27],\n",
       "         [ 1,  1, 21,  9,  6,  1,  1, 16,  7,  1,  1,  2, 15,  5,  1],\n",
       "         [15,  5, 10,  4,  2, 21, 16, 19, 20,  1,  1, 20,  6,  2, 13],\n",
       "         [19, 26,  5,  2, 26,  1,  1,  2, 17, 17,  2, 19,  6, 15, 21],\n",
       "         [14,  2,  1,  1, 15,  6, 25, 22, 20,  1,  1,  4,  2, 15,  4],\n",
       "         [ 6, 13,  6,  4, 21, 19, 16, 15, 10,  4,  2, 13, 13, 26,  1],\n",
       "         [20,  1,  1, 20,  9, 22,  7,  7, 13,  6,  1,  1, 24,  2, 19]]),\n",
       "  array([[10,  8, 15,  6,  5,  1,  1,  2, 13, 16, 15,  8,  1,  1,  2],\n",
       "         [21, 10,  4,  6, 20,  1,  1, 20, 16, 19, 21,  6,  5,  1,  1],\n",
       "         [21,  6, 19, 20,  1,  1,  2, 20, 21,  6, 19, 10, 20, 12,  1],\n",
       "         [13, 13, 26,  1,  1,  7,  2, 21,  9,  6, 19,  1,  1,  6, 13],\n",
       "         [ 6, 15, 20,  1,  1, 20, 21,  6, 23,  6, 15, 20, 16, 15,  1],\n",
       "         [19,  6,  4, 21, 15,  6, 20, 20,  1,  1, 24,  6, 20, 21,  4],\n",
       "         [10, 20,  9,  6,  5,  1,  1, 11, 16, 15,  2, 20,  1,  1,  2],\n",
       "         [ 1,  7,  2, 23, 16, 19,  1,  1, 23,  6, 21,  6, 19,  2, 15],\n",
       "         [ 4,  1,  1,  4, 19,  2, 20,  9,  6, 20,  1,  1, 20, 21,  2],\n",
       "         [13, 16,  2,  5, 20,  1,  1,  7, 19, 10,  6, 15,  5, 20,  9],\n",
       "         [10, 19,  6,  1,  1, 17, 16, 17, 22, 13,  2, 21, 10, 16, 15],\n",
       "         [ 1,  3,  2,  4, 12, 20,  1,  1, 16,  3, 20,  6, 19, 23,  6],\n",
       "         [16, 16, 12,  6,  5,  1,  1,  5, 10, 20,  4, 22, 20, 20, 10],\n",
       "         [ 1,  1,  4,  9, 10, 15,  2,  1,  1, 14,  2, 12, 10, 15,  8],\n",
       "         [16, 19, 15,  6, 19,  1,  1, 19,  2, 15, 12,  1,  1,  4, 16],\n",
       "         [ 4, 16, 16, 12,  6,  5,  1,  1, 21,  9, 19, 10, 13, 13,  6],\n",
       "         [ 6, 19, 20, 16, 15,  1,  1, 19,  2, 26, 20,  1,  1,  2, 20],\n",
       "         [19,  2, 15, 20, 17, 16, 19, 21,  6, 19,  1,  1, 11, 16, 13],\n",
       "         [ 1,  1, 20, 16, 15, 16, 14,  2,  1,  1,  6, 20, 21,  2,  1],\n",
       "         [ 5, 16, 24, 15, 13, 16,  2,  5,  1,  1,  9,  1,  1,  9, 10],\n",
       "         [10, 16, 15,  1,  1, 20, 13, 16, 21, 20,  1,  1, 10, 15,  4],\n",
       "         [20, 10, 15,  8,  2, 17, 16, 19,  6,  1,  1,  6, 15, 21,  6],\n",
       "         [ 9, 22,  3,  3, 26,  1,  1,  8, 19,  2, 23,  6,  1,  1, 13],\n",
       "         [ 1,  1, 20, 15,  2, 12,  6, 20,  1,  1, 20, 24,  6, 13, 13],\n",
       "         [15,  1,  1,  4, 16, 17, 10,  6, 19,  1,  1, 22, 15,  4,  2],\n",
       "         [ 1,  1, 21, 19, 22, 20, 21,  1,  1, 23,  2, 15,  1,  1,  4],\n",
       "         [23,  6, 15, 10,  2, 15,  1,  1, 17, 19,  2,  4, 21, 10,  4],\n",
       "         [21, 10, 13, 10, 21, 26,  1,  1,  3, 26, 19,  5,  1,  1, 20],\n",
       "         [15,  1,  1,  7, 16, 22, 15,  5, 10, 15,  8,  1,  1, 20, 24],\n",
       "         [ 1,  1, 17,  2,  4,  6,  5,  1,  1,  6, 15,  8, 13,  1,  1],\n",
       "         [15,  1,  1, 24, 10, 12, 10, 17,  6,  5, 10,  2,  1,  1,  4],\n",
       "         [ 6,  1,  1,  8, 22,  2, 19,  5, 20,  1,  1,  2, 15, 15, 16],\n",
       "         [20, 15, 21,  1,  1, 24,  2, 23,  6,  7, 16, 19, 14,  1,  1],\n",
       "         [19,  1,  1, 20, 21,  2, 19,  1,  1,  2, 19,  6,  2, 20,  1],\n",
       "         [19, 21, 10, 15,  8,  1,  1, 21, 10,  3,  6, 21,  2, 15,  1],\n",
       "         [ 1,  5, 16, 19, 16, 21,  9, 26,  1,  1,  7, 19,  6,  6, 14],\n",
       "         [17, 13,  2,  4,  6,  5,  1,  1, 20, 16, 22, 15,  5,  6,  5],\n",
       "         [ 1, 19,  6,  4, 16, 14,  3, 10, 15,  2, 15, 21,  1,  1, 22],\n",
       "         [10,  5,  6,  1,  1, 11,  1,  1,  7, 16, 16,  5,  1,  1, 20],\n",
       "         [ 1,  1,  3,  6,  9,  2, 23, 10, 16, 19,  2, 13,  1,  1, 19],\n",
       "         [25,  6, 20,  1,  1, 17, 16, 20, 21,  7, 10, 25,  1,  1, 20],\n",
       "         [ 4,  9,  2,  3, 13,  6,  1,  1, 21, 16, 22,  4,  9, 10, 15],\n",
       "         [ 1, 23, 10, 23,  2,  1,  1, 21, 19,  6,  2,  5, 14, 10, 13],\n",
       "         [ 6, 19,  1,  1, 10, 13, 13, 10, 15, 16, 10, 20,  1,  1, 19],\n",
       "         [ 5,  6, 14,  6, 15, 21, 10,  2,  1,  1,  3,  2, 19, 13,  6],\n",
       "         [ 9, 19, 10, 20,  1,  1,  2, 21, 21,  6, 15, 21, 10, 16, 15],\n",
       "         [19, 10,  5,  6, 19, 20,  1,  1,  4,  9,  2, 19,  8,  6, 19],\n",
       "         [ 3, 13,  6,  1,  1,  2, 13, 13, 16,  4,  2, 21, 10, 16, 15],\n",
       "         [ 1,  2, 15, 21, 10,  8, 22,  2,  1,  1, 15, 16, 21, 21, 10],\n",
       "         [20,  1,  1,  4, 16, 15, 15,  6,  4, 21, 16, 19, 20,  1,  1],\n",
       "         [ 9, 10, 15,  8,  1,  1, 20, 17, 16, 19, 21, 10, 15,  8,  1],\n",
       "         [ 8, 20,  1,  1, 10, 14, 20,  1,  1, 14, 16, 10, 20, 21,  1],\n",
       "         [10,  3, 22, 21, 10, 16, 15,  1,  1, 14,  6,  2, 20, 22, 19],\n",
       "         [13,  6,  1,  1,  7, 22, 19, 15, 10, 20,  9, 10, 15,  8, 20],\n",
       "         [ 1, 21, 10,  1,  1,  7,  4,  1,  1,  8,  6, 13,  1,  1, 21],\n",
       "         [21,  9, 16, 22, 20,  6,  1,  1, 17, 19, 16, 23,  6, 20,  1],\n",
       "         [ 5,  1,  1,  2, 13,  3,  2,  1,  1, 22, 15,  5,  6, 19, 24],\n",
       "         [15,  8,  1,  1,  4, 16, 14, 17, 22, 21,  6, 19, 10, 27,  6],\n",
       "         [ 1, 21,  9,  6,  1,  1, 16,  7,  1,  1,  2, 15,  5,  1,  1],\n",
       "         [ 5, 10,  4,  2, 21, 16, 19, 20,  1,  1, 20,  6,  2, 13,  6],\n",
       "         [26,  5,  2, 26,  1,  1,  2, 17, 17,  2, 19,  6, 15, 21, 13],\n",
       "         [ 2,  1,  1, 15,  6, 25, 22, 20,  1,  1,  4,  2, 15,  4,  6],\n",
       "         [13,  6,  4, 21, 19, 16, 15, 10,  4,  2, 13, 13, 26,  1,  1],\n",
       "         [ 1,  1, 20,  9, 22,  7,  7, 13,  6,  1,  1, 24,  2, 19,  5]]))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "list(dataset.take(1).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f82b54",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "We use an RNN with GRUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c0999b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "rnn_units = 257\n",
    "\n",
    "class RnnGRUModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "    \n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x\n",
    "\n",
    "  def get_config(self):\n",
    "    return {\n",
    "        \"embedding\": self.embedding,\n",
    "        \"gru\": self.gru,\n",
    "        \"dense\": self.dense,\n",
    "    }\n",
    "\n",
    "  @classmethod\n",
    "  def from_config(cls, config):\n",
    "    return cls(**config)\n",
    "\n",
    "\n",
    "\n",
    "model = RnnGRUModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5382190",
   "metadata": {},
   "source": [
    "### Try the untrained model on the first sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "396fea63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 15, 28) # (batch_size, sequence_length, vocab_size)\n",
      "Model: \"rnn_gru_model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      multiple                  3584      \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  multiple                  298377    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              multiple                  7224      \n",
      "=================================================================\n",
      "Total params: 309,185\n",
      "Trainable params: 309,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "    \n",
    "# vocab size is 28 because of [UNK]\n",
    "# TODO we don't actually need [UNK] since input involves all possible characters already\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "5cedc587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18,  3,  2,  3,  5,  4, 18, 27,  4, 19,  0, 19,  4,  2,  9])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "aa643e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " sors**fox**unli\n",
      "Next Char Predictions:\n",
      " qbabdcqzcr[UNK]rcah\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy().decode('utf-8'))\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "2891ae18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 15, 28)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         3.3323288\n"
     ]
    }
   ],
   "source": [
    "# Could try with other loss functions, e.g. CosineSimilarity\n",
    "\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "mean_loss = example_batch_loss.numpy().mean()\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", mean_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7f8784",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "9f2ce834",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "a189390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = '20k_training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "36afa12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e41138f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 10:21:13.235064: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-17 10:21:13.426814: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4/171 [..............................] - ETA: 3s - loss: 3.3107  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 10:21:13.479811: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171/171 [==============================] - 4s 19ms/step - loss: 2.6625\n",
      "Epoch 2/10\n",
      "171/171 [==============================] - 3s 19ms/step - loss: 2.3891\n",
      "Epoch 3/10\n",
      "171/171 [==============================] - 3s 19ms/step - loss: 2.3552\n",
      "Epoch 4/10\n",
      "171/171 [==============================] - 3s 19ms/step - loss: 2.2358\n",
      "Epoch 5/10\n",
      "171/171 [==============================] - 3s 19ms/step - loss: 2.1513\n",
      "Epoch 6/10\n",
      "171/171 [==============================] - 3s 19ms/step - loss: 2.1109\n",
      "Epoch 7/10\n",
      "171/171 [==============================] - 3s 19ms/step - loss: 2.0793\n",
      "Epoch 8/10\n",
      "171/171 [==============================] - 3s 19ms/step - loss: 2.0417\n",
      "Epoch 9/10\n",
      "171/171 [==============================] - 3s 19ms/step - loss: 2.0117\n",
      "Epoch 10/10\n",
      "171/171 [==============================] - 3s 19ms/step - loss: 1.9872\n"
     ]
    }
   ],
   "source": [
    "# TODO: add a callback to print out generated words at a point in training\n",
    "# TODO: add a callback to print out how the model evaluates a particular word at each point in training\n",
    "\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6deae8",
   "metadata": {},
   "source": [
    "### Create ModelWrapper which samples from the model one step at a time and evaluates words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7d02212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapper():\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=0.5):\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states\n",
    "\n",
    "  def probability_of_letter(this, logits, letter):\n",
    "    return (tf.squeeze(logits)[this.ids_from_chars(letter).numpy()]).numpy()\n",
    "\n",
    "  def evaluate_word(this, word, show_work):\n",
    "      # Seed the model with '**'\n",
    "      seed_chars = tf.constant(['**'])\n",
    "      input_chars = tf.strings.unicode_split(seed_chars, 'UTF-8')\n",
    "      input_ids = this.ids_from_chars(input_chars).to_tensor()\n",
    "      states = None\n",
    "\n",
    "      # get the initial probability distribution\n",
    "      predicted_logits, states = this.model(inputs=input_ids, states=states, return_state=True)\n",
    "      predicted_logits = predicted_logits[:, -1, :]/this.temperature\n",
    "\n",
    "      total_prob = 0\n",
    "      for letter in word:\n",
    "        # check the log probability of the letter given current state\n",
    "        probability = this.probability_of_letter(predicted_logits, letter)\n",
    "        total_prob += probability\n",
    "        if show_work:\n",
    "          print(\"Probability of\", letter, \"=\", probability)\n",
    "\n",
    "        # feed this letter into the model\n",
    "        this_char = tf.constant([letter])        \n",
    "        input_char = tf.strings.unicode_split(this_char, 'UTF-8')\n",
    "        this_input_id = this.ids_from_chars(input_char).to_tensor()\n",
    "        predicted_logits, states = this.model(inputs=this_input_id, states=states,\n",
    "                                              return_state=True)\n",
    "        predicted_logits = predicted_logits[:, -1, :]/this.temperature\n",
    "\n",
    "      # check the log probability of '*' given current state\n",
    "      probability = this.probability_of_letter(predicted_logits, '*')\n",
    "      # TODO This assumes that the probability of the word is the product of the probability of each letter\n",
    "      # the problem is that I'm not really training the RNN to generate probabilities for each letter. In otherwords\n",
    "      # the log likelihoods in the logits is not real, just directionally correct\n",
    "      total_prob += probability\n",
    "      if show_work:\n",
    "        print(\"Probability of * =\", probability)\n",
    "\n",
    "      # TODO normalize by length\n",
    "      # TODO actually, shouldn't the model do this automatically if it's well trained?\n",
    "      return total_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c607d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrapper = ModelWrapper(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "27958a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 10:21:53.007935: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-17 10:21:53.032555: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-17 10:21:53.128436: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-17 10:21:53.151722: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**charize*\n",
      "**charmand*\n",
      "**chare*\n",
      "**charrica*\n",
      "**charity*\n",
      "**charture*\n",
      "**charger*\n",
      "**charl*\n",
      "**chargers*\n",
      "**charrie*\n"
     ]
    }
   ],
   "source": [
    "def create_words(given_model, num_words):\n",
    "    for m in range(num_words):\n",
    "      states = None\n",
    "      next_char = tf.constant(['**char'])\n",
    "      result = [next_char]\n",
    "\n",
    "      for n in range(100):\n",
    "        next_char, states = given_model.generate_one_step(next_char, states=states)\n",
    "        result.append(next_char)\n",
    "        if next_char == '*':\n",
    "          break\n",
    "\n",
    "      result = tf.strings.join(result)\n",
    "      end = time.time()\n",
    "      print(result[0].numpy().decode('utf-8'))\n",
    "\n",
    "create_words(model_wrapper, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow25] *",
   "language": "python",
   "name": "conda-env-tensorflow25-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
